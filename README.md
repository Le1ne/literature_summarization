## Реферирование художественной литературы посредством больших языковых моделей

Работа посвящена исследованию подходов к автоматическому реферированию художественных текстов с использованием больших языковых моделей. Художественные тексты, в отличие от научно-технической литературы, характеризуются высокой степенью стилистической и семантической сложности, что делает задачу реферирования особенно трудной. Серьёзным ограничением является контекстное окно современных моделей, не позволяющее обрабатывать длинные произведения полностью. В таких условиях методы, позволяющие эффективно сжимать текст без потери смысла содержания, становятся перспективным решением в задаче реферирования.

## Требования

Библиотеки Python:
- `asyncio`
- `openai`
- `scipy`
- `nltk`
- `transformers`
- `tqdm`
- `evaluate`
- `sentence_transformers`
- `ignite`
- `numpy`

Для работы с LLM требуется клиент, подключающийся к серверу, на котором развернуты LLM. Для работы клиента требуются API-ключ и URL.

В файле `utils.py` содержатся необходимые реализации клиента для асинхронного доступа к LLM, а также вспомогательные функции.

## Сбор датасета

В папке `dataset collection` содержатся файлы, производящие парсинг HTML-страниц, находящихся в `raw pages`. Обработанные с помощью LLM аннотации направляются из папки `unprocessed annotations` в `processed annotations`

В ходе сборки дубликаты и страницы, ссылающиеся на другую страницу, были удалены.

## Методы

В папке `methods` содержатся методы, используемые в ходе работы:
- Псевдо-генерация
- Иерархический
- Итеративный
- "Чертёжный" (Text-Blueprint)
- Иерархический с фильтрацией узлов
- "Чертёжный" с кластеризацией вопросов

## Метрики

Оценить качество сгенерированных аннотаций можно с помощью базовых метрик ROUGE-L, BERTScore (в работе в качестве модели для подсчёта использовалась `USER-bge-m3`) и метрик покрытия ключевых вопросов (Coverage) и схожести ответов на ключевые вопросы (Answer Similarity).

**Coverage**: метрика, оценивающая долю покрытия аннотацией заранее сгенерированных ключевых вопросов, на которые содержится ответ в аннотации.
- Ключевые вопросы генерируются заранее с помощью мощной LLM (`Meta-Llama-3-70B`) по эталонному тексту и фиксируются для всех аннотаций.
- Для каждого ключевого вопроса в LLM подаётся запрос: «Есть ли ответ на этот вопрос в тексте аннотации?»
- Ответ должен начинаться с да/нет, после чего используется оценка вероятности положительного ответа.
- Если вероятность превышает 0.75, считается, что вопрос покрыт.

$$
\text{Coverage} = \frac{\text{Число покрытых вопросов}}{\text{Общее число вопросов}}
$$

**Answer Similarity**: среднее семантическое сходство между краткими ответами на ключевые вопросы из сгенерированных и эталонных аннотаций.
- Используются те же ключевые вопросы, что и в метрике полноты покрытия.
- Для каждого вопроса заранее генерируется краткий эталонный ответ (одно предложение или до 10 слов) по эталонной аннотации.
- Если вопрос не покрывается сгенерированной аннотацией (по предыдущей метрике), то сходство считается равным 0.0.
- Если покрывается, то LLM генерирует краткий ответ на основе сгенерированной аннотации, и вычисляется семантическое сходство между этим ответом и эталонным.
- Сходство рассчитывается с помощью SentenceTransformer (`USER-bge-m3`).

$$
\text{AnswerSimilarity} = \frac{1}{N} \sum_{i=1}^{N} \text{sim}(a_i^\text{pred}, a_i^\text{ref}),
$$
где $\text{sim}$ — косинусное сходство между эмбеддингами ответов.

Все метрики реализованы в файле `metrics.py`.
