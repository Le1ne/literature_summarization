## Реферирование художественной литературы посредством больших языковых моделей

Работа посвящена исследованию подходов к автоматическому реферированию художественных текстов с использованием больших языковых моделей. Художественные тексты, в отличие от научно-технической литературы, характеризуются высокой степенью стилистической и семантической сложности, что делает задачу реферирования особенно трудной. Серьёзным ограничением является контекстное окно современных моделей, не позволяющее обрабатывать длинные произведения полностью. В таких условиях методы, позволяющие эффективно сжимать текст без потери смысла содержания, становятся перспективным решением в задаче реферирования.

## Требования

Библиотеки Python:
- `asyncio`
- `openai`
- `scipy`
- `nltk`
- `transformers`
- `tqdm`
- `evaluate`
- `sentence_transformers`
- `ignite`
- `numpy`

Для работы с LLM требуется клиент, подключающийся к серверу, на котором развернуты LLM. Для работы клиента требуются API-ключ и URL.

В файле `utils.py` содержатся необходимые реализации клиента для асинхронного доступа к LLM, а также вспомогательные функции.

## Сбор датасета

В папке `dataset collection` содержатся файлы, производящие парсинг HTML-страниц, находящихся в `raw pages`. Обработанные с помощью LLM аннотации направляются из папки `unprocessed annotations` в `processed annotations`

В ходе сборки дубликаты и страницы, ссылающиеся на другую страницу, были удалены.

## Методы

В папке `methods` содержатся методы, используемые в ходе работы:
- Псевдо-генерация
- Иерархический
- Итеративный
- "Чертёжный" (Text-Blueprint)
- Иерархический с фильтрацией узлов
- "Чертёжный" с кластеризацией вопросов

## Метрики

Оценить качество сгенерированных аннотаций можно с помощью базовых метрик ROUGE-L, BERTScore (в работе в качестве модели для подсчёта использовалась `USER-bge-m3`) и метрик покрытия ключевых вопросов (Coverage) и схожести ответов на ключевые вопросы (Answer Similarity).

Coverage: метрика, оценивающая долю покрытия аннотацией заранее сгенерированных ключевых вопросов, на которые содержится ответ в аннотации.
- Ключевые вопросы генерируются заранее с помощью мощной LLM (`Meta-Llama-3-70B`) по эталонному тексту и фиксируются для всех аннотаций.
- Для каждого ключевого вопроса в LLM подаётся запрос: «Есть ли ответ на этот вопрос в тексте аннотации?»
- Ответ должен начинаться с да/нет, после чего используется оценка вероятности положительного ответа.
- Если вероятность превышает 0.75, считается, что вопрос покрыт.
$ Coverage = Число покрытых вопросов / Общее число вопросов $

